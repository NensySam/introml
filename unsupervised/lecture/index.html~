<!DOCTYPE html>
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width">
<style>

@import url(http://fonts.googleapis.com/css?family=Yanone+Kaffeesatz:400,700);
@import url(http://fonts.googleapis.com/css?family=Contrail+One:400,700);

</style>
<link rel="stylesheet" type="text/css" href="../../library/common.css" />
<link rel="stylesheet" type="text/css" href="../../library/screen.css" media="screen" />
<link rel="stylesheet" type="text/css" href="../../library/print.css" media="print" />
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js"></script>

</head>
<section style="text-align:center;padding-top:5em;">
  <h1><span class="green">Introduction à la data-science et à ses outils   </span></h1></br> <h2><p class="grey">Master, Paris Est MLV</h2>
  <a href="http://www.twiter.com/comeetie">@comeetie</a>
</section>



<section style="text-align:center;padding-top:5em;">
  <h1><span class="green">Clustering,</span></h1></br> <h2>Trouver des groupes homogènes<br> <p class="grey">Master, Paris Est-MLV</h2>
  <a href="http://www.twiter.com/comeetie">@comeetie</a>
</section>

<section style="background-color:white;color:black">

<h1 class="green" >Clustering ?</h1>


<h3>Touver des groupes <span class="red">"homogènes"</span></h3>

<ul>
<li> Regroupement automatique, 
<li> Classification automatique
<li> Apprentissage non supervisé
<li> Données : 
</ul>
$$\{\mathbf{x}_1,...,\mathbf{x}_N\}, \mathbf{x_i} \in \mathcal{R}^p$$

</section>
<section style="background-color:white;color:black">

<h1 class="green" >Clustering ?</h1>


<h3>Touver des groupes <span class="red">"homogènes"</span></h3>

<ul>
<li> Regroupement automatique, 
<li> Classification automatique
<li> Apprentissage non supervisé
<li> Données : 
</ul>
<img src="./images/plot_cluster.png" height=30%>
<h4 style="text-align:center">Illustration de quelques exemples de clustering</h4>
</section>


<section style="background-color:white;color:black">
<h1 class="blue"> Comment mesurer l'homogénité (1)</h1>
Distance moyenne au centre, variance :
$$\sum_{i=1}^{N}||\mathbf{x}_i-\mu||^2$$
Groupe homogènes, faible variance autour des centres de chaque groupe
$$\sum_{i=1}^N\sum_{k=1}^{K}z_{ik}||\mathbf{x}_i-\mu_k||^2$$
avec : $\forall i\in \{1,...N\},\, z_{ik}\in\{0,1\}^K,\,\sum_{k=1}^{K}z_{ik}=1$
</section>

<section style="background-color:white;color:black">
<h1 class="blue"> K-means</h1>
Minimisation de la variance intra-groupe :
$$z_1,...,z_N\,et\,\mathbf{\mu_1},...,\mathbf{\mu_K}=\arg\min\sum_{i=1}^N\sum_{k=1}^{K}z_{ik}||\mathbf{x}_i-\mu_k||^2$$
avec : $\forall i\in \{1,...N\},\, z_{ik}\in\{0,1\}^K,\,\sum_{k=1}^{K}z_{ik}=1$
<p>
<h3 class="red">Problème :</h3>
Optimisation non convexe, pas d'algorithme polynomial</p>
</section>

<section style="background-color:white;color:black">
<h1 class="blue"> K-means</h1>
Minimisation de la variance intra-groupe :
$$\arg_{z_1,...,z_N}\min_{\mathbf{\mu_1},...,\mathbf{\mu_K}}\sum_{i=1}^N\sum_{k=1}^{K}z_{ik}||\mathbf{x}_i-\mu_k||^2$$
<h3 class="red">Solution :</h3>
Optimisation alternée :
<ul>
<li> Pour $z_1,...z_N$ fixés, facile d'optimiser par rapport à $\mu_1,...,\mu_k$ simple calcul de moyennes
<li> Pour $\mu_1,...,\mu_K$ fixés, facile d'optimiser par rapport à $z_1,...,z_N$ affectation au centre le plus proche
<h3 class="red">Algorithme itératif convergeant vers un optima local</h3>
</section>




<section style="background-color:white;color:black">
<h1 class="blue"> K-means example</h1>
<iframe width="100%" height="70%" scrolling="no" src="./kmeans.html" style="border: none;"></iframe>
<h4 style="text-align:center"><a href="http://bl.ocks.org/jbeuckm/raw/5731501/">http://bl.ocks.org/jbeuckm/raw/5731501/</a> </h4>
</section>

<section style="background-color:white;color:black">
<h1 class="blue"> K-means</h1>
Minimisation de la variance intra-groupe :
$$\arg_{z_1,...,z_N}\min_{\mathbf{\mu_1},...,\mathbf{\mu_K}}\sum_{i=1}^N\sum_{k=1}^{K}z_{ik}||\mathbf{x}_i-\mu_k||^2$$
<h3 class="red">Remarques : </h3>
<ul>
<li> Nécessite de fixer $K$
<li> Fait implicitement l'hypothèse que les points sont réparties sous forme de "boules" autour de "K" centres
<li> Fait implicitement l'hypothèse que les groupes sont de tailles comparables
</ul>
</section>

<section style="background-color:white;color:black">
<h1 class="green"> Comment mesurer l'homogénité (2)</h1>
<ul>
<li>Groupe homogène = groupe de points similaires
<li>Deux points sont similaires si ils sont proches
<li>Deux groupes de points $(C_1,C_2)$ sont similaires si ?
</ul>
La distance maximale entre deux menbres des groupes est faible :
$$ max_{i \in C_1,j\in C_2}||x_i-x_j||^2$$
<h3 class="green">Lien maximal </h3>
</section>



<section style="background-color:white;color:black">
<h1 class="red"> Comment mesurer l'homogénité (2)</h1>
<ul>
<li>Groupe homogène = groupe de points similaires
<li>Deux points sont similaires si ils sont proches
<li>Deux groupes de points $(C_1,C_2)$ sont similaires si ?
</ul>
La distance minimale entre deux menbres des groupes est faible :
$$ min_{i \in C_1,j\in C_2}||x_i-x_j||^2$$
<h3 class="green">Lien minimal </h3>
</section>

<section style="background-color:white;color:black">
<h1 class="green"> Comment mesurer l'homogénité (2)</h1>
<ul>
<li>Groupe homogène = groupe de points similaires
<li>Deux points sont similaires si ils sont proches
<li>Deux groupes de points $(C_1,C_2)$ sont similaires si ?
</ul>
La distance moyenne entre deux menbres des groupes est faible :
$$ \frac{1}{N_{C_1}N_{C_2}}\sum_{i \in C_1}\sum_{j\in C_2}||x_i-x_j||^2$$
<h3 class="green">Lien moyen </h3>
</section>

<section style="background-color:white;color:black">
<h1 class="green"> Comment mesurer l'homogénité (2)</h1>
<ul>
<li>Groupe homogène = groupe de points similaires
<li>Deux points sont similaires si ils sont proches
<li>Deux groupes de points $(C_1,C_2)$ sont similaires si ?
</ul>
La distance(pondérés par les effectifs) entre leurs moyennes est faible :
$$ \frac{N_{C_1}N_{C_2}}{N_{C_1}+N_{C_2}}||\mu_{C_1}-\mu_{C_2}||^2$$
<h3 class="green">Distance de ward</h3>
</section>

<section style="background-color:white;color:black">
<h1 class="green">CAH</h1>
<ul>
<li> Placer chaque points dans un groupe dont il est l'unique membre
<li> Regroupper itérativement les deux groupes de points les plus proches au sens d'une distance d'aggrégation 
<li> Jusqu'à n'avoir plus qu'un groupe
<li> Construction d'un arbre (binaire) de regroupement (dendograme) 
<li> Couper l'abre à une certaine auteur pour obtenir un clustering 
</ul>
</section>


<section style="background-color:white;color:black">
<h1> CAH example</h1>
<iframe width="100%" height="70%" scrolling="no" src="./agglo.html" style="border: none;"></iframe>
<h4 style="text-align:center"><a href="./agglo.html">"./agglo.html"</a> </h4>
</section>

<section style="background-color:white;color:black">
<h1 class="green">CAH</h1>
Minimisation gloutone du critère de fusion
<h3 class="red">Remarques : </h3>
<ul>
<li> Ne nécessite pas de fixer $K$ a priori
<li> L'étude du dendograme peut aider à fixer $K$
<li> Choix du critère d'aggrégation important seul paramètre de l'algorithme; peut mener à des résultats très différents
<li> Pas d'hypothèse forte sur la forme des groupes
</ul>
</section>

<section style="background-color:white;color:black">
<h1 class="red"> Comment mesurer l'homogénité (3)</h1>
<ul>
<li>Groupe de points appartement à la même zone de haute densité
</ul>
<h3 class="green">Densité ~ nombre de point par unité de surface</h3>
</section>


<section style="background-color:white;color:black">
<h1 class="red"> Comment mesurer l'homogénité (3)</h1>
<ul>
<li>Groupe de points appartement à la même zone de haute densité
</ul>
<h3 class="green">Densité ~ nombre de point par unité de surface</h3>
Mesure de densité locale $\xi(\mathbf{x},\epsilon) = \sum_{i=1}^{N}1_{\{||\mathbf{x}-\mathbf{x}_i\||^2<\epsilon\}}$
<ul>
<li>Zone de forte densité :
 $$\xi(\mathbf{x},\epsilon) > minpts $$
<li>$i,j$ dans la même zone de forte densité si :
<p>il existe un chemin de $i$ à $j$ le long duquel la densité est toujours élevée </p>
</ul>
</section>

<section style="background-color:white;color:black">
<h1 class="red">DBSCAN </h1>
paramètres :$\epsilon, minpts$</br>
nbcluster = 0
<ul>
<li> Pour chaque $\mathbf{x_i}$ :
<ul> 
<li> Si $\mathbf{x_i}$ a été visité poursuivre;
<li> Sinon :</br>

<ul>
<li> Si $\xi(\mathbf{x}_i,\epsilon)>minpts$ </br>
nbcluster++</br>
étendre($\mathbf{x}_i$,nbcluster)</br>
<li> Sinon placer $\mathbf{x}_i$ dans le cluster de bruit : $c_i=0$
</section>

<section style="background-color:white;color:black">
<h1 class="red">DBSCAN</h1>
étendre($\mathbf{x}_i$,nbcluster)</br>
<ul>
<li> $c_i=nbcluster$
<li> $Voisins = \{\mathbf{x}_j :\, ||\mathbf{x}_j-\mathbf{x}_i||^2<\epsilon\}$
<li> Pour chaque $\{\mathbf{x}_j\, \in\, Voisins\}$
<ul> 
<li> Si $\mathbf{x}_j$ n'a pas été visité
<ul>
<li> Marquer $\mathbf{x_j}$ comme visité
<li> Si $\xi(\mathbf{x}_j,\epsilon)>minpts$ </br>
$$Voisins = Voisins \cup \{\mathbf{x}_l :\, ||\mathbf{x}_l-\mathbf{x}_j||^2<\epsilon\}$$ 
<li> Si $\mathbf{x}_j$ n'est menbre d'aucun cluster $c_j=nbcluster$
</section>

<section style="background-color:white;color:black">
<h1 class="red">DBSCAN</h1>
<h3>Remarques</h3>
<ul>
<li> Basé sur la densité pas d'hypothèse sur la forme des clusters
<li> Deux paramètres à définir mais pas le nombre de clusters
<li> Peut rejeter des points et ne pas les intégrer à un groupe
<li> Rapide
<ul> 
</section>


<section style="background-color:white;color:black">
<h1 class="green">Clustering</h1>
<h3>Remarques générales :</h3>
<ul>
<li> <span class="red">! au prétraitements sur les données : centrage - réduction, normalisation, ... </span>
<li> <span class="red">! à la distance </span>
<li> <span class="red">Visualisation des résultats</span>
<ul> 
</section>


<section style="background-color:white;color:black">
<h1 class="green">Clustering en python</h1>
<h3><a href="http://scikit-learn.org/stable/modules/clustering.html">Scikit Learn</a></h3>
<h3>Exemple, images de chiffre manuscrits MNIST : </h3>
<ul>
<li><a href="./notebooks/ClusteringMNIST.html">notebook (html)</a>
<li><a href="./notebooks/ClusteringMNIST.ipynb">notebook (ipynb)</a>
</ul>
</section>


<section class="exercice">
<h1>Clustering</h1>
Tester différents algorithmes de clustering sur les profils de remplissage des stations Vélib et interpréter les résultats.
<h3 class="red">! à la mise en forme des données</h3>
<h3><a href="http://vlsstats.ifsttar.fr/data/spatiotemporalstats_Paris.json">Données Vélib'</a></h3>
</section>

<script src="../../library/d3.v3.min.js"></script>
<script src="../../library/stack.v1.min.js"></script>
<link rel="stylesheet" href="../../library/styles/hybrid.css">
<script src="../../library/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>



<script>

var mystack = stack()
    .on("activate", activate)
    .on("deactivate", deactivate);

var section = d3.selectAll("section"),
    follow = d3.select("#follow"),
    followAnchor = d3.select("#follow-anchor"),
    lorenz = d3.select("#lorenz"),
    followIndex = section[0].indexOf(follow.node()),
    lorenzIndex = section[0].indexOf(lorenz.node());

function refollow() {
  followAnchor.style("top", (followIndex + (1 - mystack.scrollRatio()) / 2 - d3.event.offset) * 100 + "%");
}

function activate(d, i) {
  if (i === followIndex) mystack.on("scroll.follow", refollow);
  if (i === lorenzIndex) startLorenz();
}

function deactivate(d, i) {
  if (i === followIndex) mystack.on("scroll.follow", null);
  if (i === lorenzIndex) stopLorenz();
}


</script>
